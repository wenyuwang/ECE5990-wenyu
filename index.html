<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="This is the report for ECE5990, final project">
    <meta name="author" content="Wen-Yu Wang,Hanchen Jin">
    <link rel="icon" href="img/icon-2.png">
    <title>ECE5990Final-Project</title>
    <style type="text/css">
        ul {list-style-type: disc; font-size: 120%; line-height: 2}
        li span {font-size: 100%; vertical-align: middle;}
    </style>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/one-page-wonder.css" rel="stylesheet">
    <link rel="stylesheet" href="highlighJS/styles/default.css">
    <script src="highlighJS/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" >
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#">Pi Game Player </a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="#objectives">Objective</a>
                    </li>
                    <li>
                        <a href="#introduction">Introduction</a>
                    </li>
                    <li>
                        <a href="#setting_up">Set Up</a>
                    </li>
                    <li>
                        <a href="#System_Design">Design</a>
                    </li>
                    <li>
                        <a href="#results">Results</a>
                    </li>
					 <li>
                        <a href="#further_improvement">Improvements</a>
                    </li>
                    <li>
                        <a href="#Code_Appendix">Appendix</a>
                    </li>
                    <li>
                        <a href="#Conclusion">Conclusion</a>
                    </li>
                    <li>
                        <a href="#acknowledgement">Acknowledgement</a>
                    </li>

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Full Width Image Header -->
    <header class="header-image">
        <div class="headline">
            <div class="container">
                <h1>Pi Game Player for iPhone!</h1>
                <h2>Wen-Yu(ww424), Hanchen(hj424)</h2>
            </div>
        </div>
    </header>

    <!-- Page Content -->
    <div class="container">

        <hr class="featurette-divider">
        <!-- Zeroth Featurette -->
        <div class="featurette" id="objectives">
            <img class="featurette-image img-circle img-responsive pull-left" src="img/ourpi.jpg" alt="Our raspberry Pi">
            <h2 class="featurette-heading">What Goals
                <span class="text-muted">We want</span>
            </h2>
            <br>
            <br>
            <br>
            <p class="lead"> - Build a robotic system to play mobile game app </p>
            <p class="lead"> - High accuracy (high scores) is guaranteed</p>
            <p class="lead"> - High speed to play the game apps is also guaranteed </p>
            <br>
            <br>


        </div>

        <hr class="featurette-divider">

        <!-- First Featurette -->
        <div class="featurette" id="introduction">
            <img class="featurette-image img-circle img-responsive pull-right" src="img/game.jpg" alt="Game app layout">
            <h2 class="featurette-heading">Get to know
                <span class="text-muted">Our Idea</span>
            </h2>
            <p class="lead">Raspberry Pi is an ideal platform to build a small robot. We get the idea from Lab3 which we can build a robotic car on a raspberry pi system by writing some controlling programming. But, in Lab3, our robot does not have “eyes”. So, here is our idea – we want to build up a robot with eyes and make it play some simple game apps, such as Lumberman. Lumberman is a mobile game app, where you need to avoid the branched when you chop the wood. We implemented the algorithm to detect where the branches are and make our raspberry pi to chop the wood happily! </p>
            <br>
            
            <p style="font-size:1.2em" >Keyword: Raspberry Pi, PiCamera, Image Processing, GPIO output control </p>

        </div>

        <hr class="featurette-divider">

        <!-- Second Featurette -->
        <div class="featurette" id="setting_up">
            <h2 class="featurette-heading">Setup environment
                <span class="text-muted">For PiCamera and Libraries</span>
            </h2>
            <p class="lead">Before we start our project, we need to set up the environment for raspberry pi.</p>
            <ul>

            <li><b><span>PiCamera</span></b>
            
            <p style="font-size:1em"><span style="background-color: #E0E0E0">python-picamera</span> is a pure Python interface to the Raspberry Pi camera module for Python 2.7 (or above) or Python 3.2 (or above). 
            <br>
            <b>First: Enable PiCamera</b>
            <br>
            Run <span style="background-color: #E0E0E0">sudo raspi-config</span> and choose in the menu to enable the pi camera. A reboot is needed after this.
            <br>
            <b>Second: Install Library for PiCamera</b>
            <br>
            Run the following commands to get the library.</p>
            <blockquote>
                $ sudo apt-get update <br>
                $ sudo apt-get install python-picamera
            </blockquote>

            <p style="font-size:1em">
            Now you can import piCamera and capture a picture!</p>
            <blockquote>
                <span style="color:#0076ac">import</span> picamera <br>
                camera = picamera.PiCamera() <br>
                camera.capture(<span style="color:#01B468">'image.jpg'</span>)

            </blockquote>
            </li>
            <li><p><b><span>Image Processing Related Libraries</span></b></p>
            Since we need our raspberry pi to "see" a scene, and we need the image-processing algorithm to help us to detect if there is a branch at specific site or not. And we need to install the libraries including <span style="background-color: #E0E0E0">SciPy</span>, <span style="background-color: #E0E0E0">Python Image Library (PIL)</span>,  <span style="background-color: #E0E0E0">matlibplot</span> , and <span style="background-color: #E0E0E0">skimage</span>.

            <p style="font-size:1em"><b>Scipy/NumPy/matplotlib</b>
            <br>
            The SciPy library is one of the core packages that make up the SciPy stack. It provides many user-friendly and efficient numerical routines such as routines for numerical integration and optimization.
            <br>
            <br>
            NumPy is the fundamental package for scientific computing with Python. It contains among other things:
                <br>- a powerful N-dimensional array object
                <br>- sophisticated (broadcasting) functions
                <br>- tools for integrating C/C++ and Fortran code
                <br>- useful linear algebra, Fourier transform, and random number capabilities
                <br>Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.
            <br>
            <br>
            matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. </p>
            <blockquote>
                $ sudo apt-get install python-numpy python-scipy python-matplotlib python-pandas python-sympy python-nose 
            </blockquote>

            <p style="font-size:1em"><b>Python Imaging Library (PIL)</b>
            <br>
            The Python Imaging Library (PIL) adds image processing capabilities to your Python interpreter. This library supports many file formats, and provides powerful image processing and graphics capabilities.
            <br>
            Here we are going to install PIL using pip, so we start with installing python-pip.</p>

            <blockquote>
                $ sudo apt-get install python-pip
            </blockquote>
            With pip installed, install the required development packages:
            <blockquote>
                $ sudo apt-get install python-dev libjpeg-dev libfreetype6-dev zlib1g-dev
            </blockquote>
            <p style="font-size:1em">After installing these packages, we have to symlink the three image libraries into /usr/lib. A symlink, which is short for symbolic link, is a special type of file that contains a reference to another file or directory. The reference is in the form of an absolute or relative path and it affects path-name resolution. To do that, type in the following commands on the terminal:</p>
            <blockquote>
                $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libfreetype.so /usr/lib/
                <br>
                $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libjpeg.so /usr/lib/
                <br>
                $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libz.so /usr/lib/
            </blockquote>
            <p style="font-size:1em">Now we are ready to install PIL. Type the following:</p>
             <blockquote>
                $ sudo pip install pil
            </blockquote>
            <p style="font-size:1em">To install Pillow (recommended), type the following:</p>
            <blockquote>
                $ sudo pip install Pillow
            </blockquote>
            <p style="font-size:1em">
			<b>Tips:</b> The total size of these libraries is about <span style="background-color: #E0E0E0">2.2G</span>. But since you have to download the compressed file and then unzip them, the total free space required on your disk/SD card is about <span style="background-color: #E0E0E0">3G</span>. Be sure to you have enough space before you start downloading by checking the space availible on your curren Linux system, following command can be used: </p>
			<blockquote>
                $ df -h
            </blockquote>
            </li>
            </ul>

        </div>

        <hr class="featurette-divider">

        <!-- Third Featurette -->
        <div class="featurette" id="System_Design">
            <img class="featurette-image img-circle img-responsive pull-left" src="img/platform0.jpg" alt="platform0">
            <h2 class="featurette-heading">Sofeware and Hardware
                <span class="text-muted">work together :)</span>
            </h2>
            <br>
            <p class="lead">Our project can be split into three categories: platform design, hardware design and software design. Platform design is to provide the spaces for our smartphone and the raspberry pi, also, make sure the relative position of the camera and our smartphone can be fixed. As for hardware design, we are building the "hands" for the raspberry pi to invoke the touch events on the capacity screen of the smartphone when it needs to move to the other side. Last but not least, the software part- this part can be analogous as the mind of our raspberry pi. We apply the image-processing algorithm to detect the branches and decide where to chop the woods on the screen. This project is lots of fun! Join us and see how each part works by scrolling down!  </p>
            <br>
            <br>
            <br>

            <ul>

            <li><p><b><span>Platform Design</span></b></p>
            <p style="font-size:1em">For the platform design, we used LEGO to build our architecture. With the flexibility of LEGO, we can rebuild the platform anytime we want when it's not fit for our aim. On this platform, we built several spaces for different parts. Mobile phone, raspberry pi, and the breadboard all have their specific areas to put in. More importantly, it can help us to fix the relative distance from the picamera and mobile phone. By this proper platform, we can eliminate some uncertainties when we restart our work each time thus make it easier to implement the image processing algorithm. </p> 
            <br>
            <br>
            <div style="text-align:center">
            <img src="img/platform1.jpg" alt="platform1">
            <br>
            <br>
            <img src="img/platform2.jpg" alt="platform2">
            <img src="img/platform3.jpg" alt="platform3">
            <img src="img/platform4.jpg" alt="platform4">
            </div>
            </li>
            <li><p><b><span>Software Design</span></b></p>
            <div style="text-align:center">
            <img src="img/schematicForsoft.jpg" alt="software schematic">
            </div>
            <p style="font-size:1em">
            The flow-chart shown above is the basic idea for software part to detect the branch and tap the screen of our mobile phone. At the initialization stage, we take 5 pictures in a row to make sure the lighting condition and white balance setting are stable for piCamera. We use the fifth photo we take to determine the threshold for our detection part. Actually, we have improved the performance of our algorithms by implementing different versions of our codes, so I will go through each version and highlight the significant improvement in each step.</p>
            <br> 
            <br>
            <p style="margin-left:1em ;font-size:1em ;" >
            <b>- First version with highest scores 40  </b>
            <br>
            The first version of our design is the slowest algorithm but with high accuracy. The idea is quite simple. Among all the steps in our implementation, deciding the values of a threshold is the trickiest part for our algorithm. In our first version of our algorithm, we stored the <span style="background-color: #E0E0E0">mean values </span> of intensities over the region of the first branches right above the head of the lumberman. The reason we use the mean values is because that at the beginning of the game, there is definitely no branch right above the head of the lumberman. Besides, the region right above the head of the lumberman contains lots of high-intensity values (white color with intensity 255), then the mean value at the site is high enough to differentiate the mean values for the intensities when there appears a branch (with green, black color which lower the intensity of the area). Therefore, we just need to calculate the differences between the mean values of the intensities of the specific region with the stored values at the beginning, and if the difference is larger than some value (we set 60 here) then we can know there is a branch. 
            <br>
            The lumberman chop the tree with correctly dodging the branches but the speed is really low so that time runs out after about 14 chops (14 points). We guess the slowest process is about taking a photo but we want to do some accurate calculation. So we used the <span style="background-color: #E0E0E0">time</span> module in Python as the timer to calculate how long does each function take when it executes in our algorithm to find out the bottleneck step which takes the most time. After such operation, we found that the time for PiCamera to take a picture requires 0.2-0.4 secs while the calculation of the mean values and differences only takes a few milliseconds. In order to speed up the speeds of the capturing photos, we set the parameter <span style="background-color: #E0E0E0">use_video_port</span> <span style="color:#7700BB">True</span> in the capturing method. However, unfortunately after this small improvement, it helped quite well - it can achieve 40 points, but the time for taking photos is still too long.
            </p>
            <blockquote>
                camera.capture(fileName, <i><span style="color:#ff8800">use_video_port</span></i>=<span style="color:#7700BB">True</span>)
            </blockquote>
            <br>
            <p style="margin-left:1em ;font-size:1em ;" >
            <b>- Second version</b>
            <br>
            In the Lumberman game app, time is a key factor to get high scores. When you response too slow, the remaining time decays much quicker while if you tap quickly and correctly the remaining time will increase. So we did lots of improvements to speed up. Here are some efforts we've tried.
            </p>
            
            <p style="margin-left:2em ;font-size:1em ;" >
            <b>Lower the resolution of the picture took</b>
            <br>
            We lowered the resolution then it can help to eliminate the number of pixels we need to consider about and make it faster to write to a file and read the image file. The resolution of the pictures for our latest version is 160 * 120.</p>

            <p style="margin-left:2em ;font-size:1em ;" >
            <b>Isolate a CPU core and try to eliminate latency from CPU</b>
            <br>
            We referred to LectureNote 35 and tried to reduce the latency from CPU by isolating a CPU core. We set the configuration like the instruction following:</p>
            <blockquote>
                Add 'isolcpus=3' to /boot/cmdline.txt
            </blockquote>
            
            <p style="margin-left:2em ;font-size:1em ;" >
            After that, everytime we run our python script, we type the following to run our code.</p>
            <blockquote>
                Taskset –c 3 python fileName.py
            </blockquote>
            <p style="margin-left:2em ;font-size:1em ;" >
            This is a good way to reduce the latency from the system and it can help to speed up for 0.03 - 0.04 seconds.
            </p>
            <p style="margin-left:2em ;font-size:1em ;" >
            <b>Using in-memory streams to store the picture instead of storing in SD card.</b>
            <br>
            The other problem we think we encountered is SD card speed limitations. In other words, we  need to capture to something faster like a network port or in-memory streams. So we utilized the <span style="background-color: #E0E0E0">stream = io.BytesIO()</span> and turned the stream to PIL (Python Image Library) object for further image-processing. Although it can help speed up for a little (0.03 faster), the time is still too long.</p>
            <blockquote>
                stream = io.BytesIO()
                <br>
                camera.capture(stream, <i><span style="color:#ff8800">format</span></i>=<span style="color:#01B468">'jpeg'</span>,<i><span style="color:#ff8800">use_video_port</span></i>=<span style="color:#7700BB">True</span>)
                <br>
                stream.seek(<span style="color:#ff2610">0</span>)
                <br>
                image = Image.open(stream).convert(<i><span style="color:#01B468">'L'</span> </i>)
                <br>
                arr = np.fromiter(<span style="color:#1c96f7">iter</span>(image.getdata()), np.uint8)
                <br>
                arr.resize(<span style="color:#ff2610">120</span>, <span style="color:#ff2610">160</span>)
            </blockquote> 
            
            <p style="margin-left:2em;">
            <b>Tap several times after taking a picture (most significant)</b> <img src="img/star.png" alt ="star"><img src="img/star.png" alt ="star">
            <br>
            This idea is inspired by the generous TA - Jinyao Ren. He suggested that we can try to tap three times when we take a picture and it can help to speed up three times. We took his good advice and implement the algorithm to detect five branches at a time and tap five times accordingly. In this implementation, we encountered several challenges. 
            </p>
            <div style="text-align:center">
            <img src="img/gameLayout.jpg" alt ="layout">
            </div>
            <p style="margin-left:2em;">
            The image shown above is the screenshot of Lumberman app. As you can see here, when we take the first image to determine the threshold values, we have to consider 10 sites and set thresholds for each site. But, how? Firstly, we need to know if there is a branch at each area or not. Secondly, based on our analysis of the intensities of the images taken by PiCamera, the intensity values can vary a lot owing to the environmental lighting condition. Which results in different intensity values between not only different layers but also left-hand side and right-hand side. Therefore, as a small summary here, we need to assign different thresholds for each site. We tried different methods to determine the thresholds and improve accuracy. Like using minimum values - because if there is a branch, it would contain the black color in the area and the intensity is 0. However as we said earlier in this session, due to various environment lighting condition, the intensities at different regions are not consistent. Using minimum or maximum can not reach high accuracy for detection a branch. At last, we came up with a great idea about this, that is using <span style="background-color: #E0E0E0">"variance"</span> values over the regions for branches. Since in the case that if there is no branch at site 1,2,3,4, the background is uniform, and the variance of the intensity values at these areas is small. When there exists a branch, the variance of the intensity values can be a large value! So in lastest version of our design, the detection method for site 0 is to find out the <span style="background-color: #E0E0E0">differences</span> between the original image and each image we take for tapping event; the detection method for site 1,2,3,4 is to use <span style="background-color: #E0E0E0">variances</span> as our threshold - if the variance at the specific area is larger than some value (we set 300 here) and it means there exists a branch. Furthermore, to fit this new detection method, we need to carefully decide the region we want to detect due to some unexpected situations - such as the screen pops up "Level x" when you reach Level x and the time-remaining strip affects the detection as well. The image shown below is the regions we decide to detect the branches.
            </p>
            <div style="text-align:center">
            <img src="img/gamelayout2.jpg" alt ="layout2">
            </div>
            <p style="margin-left:2em;">
            With this modified detection method and good hardware part for tapping the capacitive screen of our mobile phone, our raspberry pi can score with 2855 - a really high new record! But there is still another issue here, that is the threshold setting is optimally designed for the red background of the game. This game contains three kinds of background, it is required to have different detection method for our algorithm. 
            </p>
            <p style="margin-left:1em;">
            <b>- Challenges for different background</b>
            <br>
            Another interesting challenges worth noted is that: in Lumberman game app there are three different background settings! This contributes to different relative values between the intensities of background and foreground, and we need to customize our code to fit in different situations.</p>
            <div style="text-align:center">
            <img src="img/bk_red.png" alt ="red background">
            <img src="img/bk_blue.png" alt ="blue background">
            <img src="img/bk_green.png" alt ="green background">
            </div>
            <br>
            
            <p style="margin-left:1em;">
            In order to deal with the issue of different background color, we analyzed the pictures before writing our code. We wrote a small helper function to help us to calculate the intensity values. We found some interesting things. First, for red background, it's easy to differentiate the branches (green) and background (red), so it is quite accurate when we utilized the variances among intensity values over the pictures. However, for the other two background settings, it's more difficult to differentiate background from foreground when we calculated the variances for intensity values over RGB. Therefore, we explored more by separating the intensity to three layers (R,G,B), and we found that if we only extract the intensity from <span style="background-color: #E0E0E0">green level</span>, the area with branch and without area are easily recognized from the calculations! That is, after extracting the G intensity values from RGB, the variance values when there is no branch will reduce to less than 100 while the variance can roar to larger than 200 when there is a branch. The reason results in this situation are that for the other two background settings, the green values contribute a lot in RGB with small R and B, and if we do the calculation by averaging RGB, it would enhance the variances owing to the big gap among these three layers. And if we only take care of G values, the high influence of less-related R values and B values can be reduced and the variance values can be obviously told apart from the site with a branch or without a branch.

            </p>
           </li>
			<li><p><b><span>Hardware Design</span></b></p>
			So after our Raspberry Pi finishes the process of taking photos and detecting branches, it is time to build "hands" for it to tap on the screen. Instead of using expensive robotic arms, we build circuits using electric switches controlled by GPIO pins to trigger the tapping on the touch screen of our smartphone. Key points are building pads for contacting with the touch-screen, circuits for controlling the tapping by normal switches, circuits for controlling the tapping by GPIO pulses and the test of the minimum tapping time. 
			<br>
			<b>(1) Tapping Pads</b>
			<br>
			The basic principle for tapping on the capacitive touch screen is to draw a few electrons from the screen. For example, when people touch the screen with fingers, they are able to draw some electrons by the circuit loop from the cell phone to the ground since people has a capacitance about 30pF~50pF. Thus for triggering a tap on the capacitive touch screen, conductor with a certain magnitude of capacitance is required. 
			<br>
			Firstly we used several layers of <span style="background-color: #E0E0E0">copper tape</span> to build the tapping pads. After many experiments, we found that the capacitance of ten layers copper tap was good to trigger the tapping accurately. However, a deadly defect of the copper tap pad is that it is unstable and hardly to be reused. It performs really good at the first time, but next time we re-stick the pad on the screen, it is not as sensitive as before. The reason is obvious. The process of tearing off these copper tape pads from the screen slightly changes the space among each layer thus intensely alters its capacitance. It is quite annoying that we have to re-build two pads for each time we restart our work. So we decide to find other stable methods to build these two pads. 
			<br>
			Then we found the <span style="background-color: #E0E0E0">one cent coin</span> (like copper foil) could also serve as a good pad for triggering the touch. Furthermore, within the wire soldered on it, it contacts really well and very stable. Here is the picture which shows these two kinds of pads, the left one is built by the copper tap and the right one is built by the one cent coin. One tricky thing here is that the distance between the pad and the screen should be very small. So every time when we restart our work, we have to bind the pads tightly on the screen by powerful black tapes or else the sensitivity of tapping will be severely influenced which leads to higher probability of missing touches. 
			<br>
				<div style="text-align:center">
                <img src = "img/electrodes.jpg" alt ="electrodes">
                </div>
			<br>
			<b>(2) Circuits using normal switches</b>
			<br>
			After resolving the problem of stable pads for touching the screen, a simple current loop for testing its basic function is essential. Since the other side of the pad has to be connected to the ground (the back of the cell phone), a ground platform made by <span style="background-color: #E0E0E0">tinfoil</span> with the same size of our cell phone helps the connection between the other side of the wire and the back of our cell phone. As the following picture (left side) shows, with the proper phone case, it contacts pretty well.Then the pad and the ground is connected separately on the two side of a switch. There is also a diagram (right side) shows the circuit of this connection. By successfully trigger the tapping by normal switches, it is time to move on electric switches.
			<br>
				<div style="text-align:center">
                <img src = "img/ground.jpg" alt ="ground">
				<img src = "img/circuit_v1.jpg" alt ="circuit_version1">
                </div>
			<br>
			<b>(3) Eletric switches</b>
			<br>
			As for the Pi Game Player, it can not perform the "touch" actions like human beings. What it can do is output pulses through GPIO pins. So we build two electric switches by H11F1. According to the datasheet of <span style="background-color: #E0E0E0">H11F1</span>, H11F1 is composed of a Gallium-Aluminum-Arsenide IRED emitting diode and a symmetrical bilateral silicon photodetector (ideal isolated FET). So if the diode on the left side is turned on, the output TERM. (pin4 and pin6) will be connected. For our circuit, two free GPIO pins are selected to output <span style="background-color: #E0E0E0">PWM</span> (pulses) for controlling the tapping. When the GPIO output high-level voltage, the diode is turned on which leads to the connection between the pad (the coin) which is bound on the capacitive touch screen and the ground of the smartphone. After a while, GPIO will output low-level voltage and the touch screen can detect an effective tapping. Thus, that is the basic principle of tapping on the capacitive screen controlled by the Pi Game Player. 
			<br>
				<div style="text-align:center">
				<img src = "img/circuit_v2.jpg" alt ="circuit_v2">
                </div>
			<br>
			By the way, the game requires the lumberman to dodge branches by selecting his position towards the trunk. So for playing this game, touches on the left side of the screen and the right side of the screen are demanding so that two pads are built. And the reason of binding them on the bottom of the smartphone is that there is only the place available to be shaded.
			<br>
			<b>(4) Tapping time requirement</b>
			<br>
			Finally, we write a basic python script for testing the output with these electric switches. This code is also in the code appendix. The basic aim of this script is to test whether these two coins can correctly trigger the tapping. From the analysis in last part, the sleep time between the high-level and low-level output is the minimum time required for the tapping. Even if Raspberry Pi is able to touch the screen in really fast speed (in a magnitude of us), the capacitive touch screen is unable to detect that kind of fast touch. Some time is required for drawing some electrons from the touch screen and calculating the position of the touching point. In general, capacitive touch screen is able to finish this process in 3ms. But actually, we are using coins and electric switches as tapping medium so that time for drawing a few electrons may become longer. After continuous testing, we found that <span style="background-color: #E0E0E0">0.03s</span> is required for triggering a correct touch. However, since we want to continuously tap five times on the screen. The debounce time between each tapping should also be seriously considered. After carefully measuring, <span style="background-color: #E0E0E0">0.05s</span> is enough for the debouncing mechanism for iPhone touch screen. However, with our Pi Game Player become strong enough--the more accurate branches detection, miss tapping sometimes happens. If we extend the tapping time, the miss tapping is assuaged but its speed decreased. So the trade-off is between the accuracy of tapping and tapping time. After several tests, we found that the total tapping time <span style="background-color: #E0E0E0">0.15s</span> (<span style="background-color: #E0E0E0">0.05s</span> tapping+<span style="background-color: #E0E0E0">0.1s</span> debouncing) is a good balance between the accuracy and speed of tapping. 
			<br>
            </li>
            </ul>
			
			<div style="text-align:center">
            <p style="font-size:2em" > Now our Pi Game Player can play the game! Come join us and see if you can beat it!! </p>
            </div>
            
        </div>

        <hr class="featurette-divider">
        <!-- Fourth Featurette -->
        <div class="featurette" id="results">
            <img class="featurette-image img-circle img-responsive pull-right" src="img/result1.jpg" alt ="result1">
            <h2 class="featurette-heading">How Good
                <span class="text-muted">We Achieved</span>
            </h2>
            <p class="lead">Until now, out powerful Pi Game Player reached the score up to 2855! It is pretty high which even beat one of the developers who is really good at this game (1000). So we think our robot can defeat any human players! Here we will introduce some timing specification of our awesome Pi Game Player to analyze why it can defeat human players quantitatively.</p>
            <br>
            <ul>
			<li>So the following picture shows the timing specification for one cycle in the main while loop. Here we executed the timer for the processes including taking photos, calculations for detections and tapping in serial.As shown in the following picture, taking one phone takes about 0.3s; the calculation takes about 3ms which are really faster compared with other two processes thus can be ignored; the five tapping takes about 0.75s. So for each tapping, our Pi Game Player takes about 0.21s and it can play this game without any wrong detection (But the reason for some wrong tapping will be discussed in next part). For human beings, the time for seeing an image and generate a signal to act is at least 0.1s; the time for taping is at least 0.1s and this can be tested by the Stopwatch on our cell phone. So even the most powerful human player can only perform one tapping in 0.21s--the same speed of our Pi game player. However, people will be tired and distracted by the environment which can lead to the misjudge of the branches so human players can't keep the highest speed--0.21s one tape for a long time. Thus, now it is impossible for any human players to defeat our Pi Game Player.</li>
            </ul>
			<br>
				<div style="text-align:center">
                <img src = "img/time.jpg" alt ="timeline">
                </div>
			<br>
        </div>
		
		<hr class="featurette-divider">
        <!-- Fourth Featurette -->
        <div class="featurette" id="further_improvement">
            <h2 class="featurette-heading">What strategies
                <span class="text-muted">To improve it</span>
            </h2>
            <p class="lead">Even if our Pi Game Player is able to defeat all human players, the accident game-over caused by miss tapping on the touch screen is an annoying but important issue for this project. If this problem can be resolved, our Pi Game Player can get an infinite score in theory. Therefore this part is going to analyze the possibilities of the missing taping and covers some methods to improve or resolve it.</p>
            <ul>
			<li><p><b><span>Possible Reasons for missing Tapping</span></b></p>
			
			The missing tapping may also result from the miss detection of the capacitive touch screen. We all have the experiences that sometimes the touch-screen is not so sensitive when we touch our smartphone. Thus, such so-called miss tapping may actually the missing detecting of our touch-screen which is unavoidable.
            </li>
			<li><p><b><span>Strategies for resolving these problems</span></b></p>
            <b>(1) Golden Pad</b>
            <br>
            To address the problem of the missing tapping owing to electrodes, TA Jacob recommended us we can use a gold pad for our electrodes since the conductivity of golden is higher than copper. So it can collect electrons from the electrodes better and faster than the copper which we are using for our project right now. 
            <br>
            <b>(2) Environmental Light Condition</b>
            <br>
            In this project, we put lots of effort in speeding up our algorithm so we made the resolution of the picture taken by the piCamera really low (160*120). However, this idea makes our algorithm vulnerable to the environmental light condition - every time we test our program we have to make the environmental light condition uniform by keeping it for a certain distance. We can make our algorithm less vulnerable by elevating our resolution to 320*240. If we make the resolution higher, the image would contain more intensity information and we can make our algorithm more stable and accurate. 
            <br>
            <b>(3) Detection </b>
            <br>
            For now, our platform uses a cheap picamera which provides a low-quality image and slows the speeds of our implementation. Another alternative to speed up our algorithm and enhance the accuracy of our detection is to use RGB Color Sensor with IR filter. With the RGB Color Sensor, it can detect the changes of the color over a region fast and accurately, which provides a better solution than piCamera. Furthermore, using this sensor, we don't need to take care of the issue of environmental light condition change because it only detects the changes in the RGB values. 
            <br>
            <b>(4) Circuits</b>
            <br>
            First, the missing tapping probably owing to the circuit we built for triggering a tapping. The most suspicious part which can cause this problem is the H11F1. According to its datasheet, the time for turning on/off is less than 15us which seems pretty good. However, here we are using coins as the pad for drawing electrons so that time consumed maybe not stable. Thus sometimes the time for drawing a few electrons maybe even more than 0.05s which leads to a miss tapping.
            <br>
            <b>(5) Parallelized Execution</b>
            <br>
			Currently, our Pi Game Player can do a good job but it can perform better. The timing control is a tricky issue here because the time for each process like taking photos, tapping can't be further decreased, and the problem of missing tapping can be reduced by the extension of tapping time. We have to consider about the enough time but can not make it too long owing to the time limit when playing the game.  The best idea for improving the speed of executing multiple processes is pipelining/paralleled execution. 
			<br>
			Here we want to appreciate our professor - Professor Joseph who provided this idea! We can build a FIFO system to store the control signals generated by the calculation. We can use different cores to be in charge different work here. For example, the job of Core 1 is to take photos, calculate the values for detection and store control signals into control FIFO. Core 2 is responsible for tapping on the screen. However, it is a tough work to implement it.
			<br>
			Following is the timing specification of the paralleled execution for our project. Here we have to analyze the timing issue carefully. Just as the timing diagram shows, after the first picture taken and analyzed, five tapping control signals have been stored into the output FIFO. The T_inter is a critical time here for starting the second photos. We want to add two more control signals in the output FIFO because the time for two tappings is nearly the same with taking one photos. By doing this, we can fill the pipeline thus get the highest throughput. We have to take the second photo with the screen is updated after two tappings and before the third tapping. As the following equation shows, T_tap is the time for the first tapping (0.15s); T_update is the time for updating the display; T_invoke is the prepareing time for taking one photo like invoking the functions.
			By analyzing these parameters, we can get the correct point to start the process of taking further photos so that the process of watching+thinking and tapping can be fluently paralleled executed. So now time for each tapping is <span style="background-color: #E0E0E0">0.15s</span>. The time contraint for this game is about <span style="background-color: #E0E0E0">0.21s</span> each tapping. In other words, within this tapping time, players can continue playing nearly forever without missing judgment. Under that condition, we can extend the tapping time to <span style="background-color: #E0E0E0">0.16~0.20s</span> to improve the tapping accuracy and improve its performance (playing speed) as well.
			<br>
				<div style="text-align:center">
                <img src = "img/time2.jpg" alt="time2.jpg">
                </div>
			<br>
			</li>
            </ul>
        </div>
		
		<hr class="featurette-divider">
                <!-- Fourth Featurette -->
        <div class="featurette" id="Code_Appendix">
            <h2 class="featurette-heading">What we have created / referenced
                <span class="text-muted">For this project</span>
            </h2>
            <p class="lead">Here is the code appendix for our project. We've created several versions of codes here.</p>
            <br>

            <p style="font-size:1.2em" ><b> Parts List </b></p>

             <table style="border: 2px solid rgb(148, 201, 255); height: 250px; background-color: rgb(242, 247, 247); width: 400px; margin: 0 auto; ">
                <tbody>
                    <tr>
                        <td><b> Name</b></td>
                        <td><b>Quantity</b></td>
                        <td><b>Price</b></td>
                    </tr>
                    <tr>
                        <td> Raspberry Pi2 B </td>
                        <td>1</td>
                        <td>35$</td>
                    </tr>
                    <tr>
                        <td> PiCamera Board Module</td>
                        <td>1</td>
                         <td>19$</td>
                    </tr>
                    <tr>
                        <td> H11F1 </td>
                        <td>2</td>
                        <td>3.3$</td>

                    </tr>
                    <tr>
                        <td> Breadboard </td>
                        <td>2</td>
                        <td>5$</td>
                    </tr>
                    <tr>
                        <td> Wires+Resistors+Tape</td>
                        <td>some</td>
                        <td>1$</td>
                    </tr>
                                        <tr>
                        <td> <b>Total </b></td>
                        <td></td>
                        <td>71.6$</td>
                    </tr>
                    </tbody>
            </table>

            <br>
            <br>
            <p style="font-size:1.2em" ><b> Codes </b></p>
            <ul>

<li><p><b><span>First version - with highest scores as 14 points </span></b></p>
            This version is our first trial, which runs slowly. The main bottle neck is the time for taking a picture. The progress of this algortithm is - take a picture, image-processing and then trigger the touch event. However, the picamera is initialized and turned on its configuration each time when it needs to capture a picture. Therefore it went slow.

<pre><code class="python">
###############################################################################
#                                                                             #
# file:    runOnPy.py                                                         #
#                                                                             #
# authors: Wen-Yu Wang  - ww424                                               #
#          Hanchen Jin  - hj424                                               #
#                                                                             #
# date:    April 30th 2016                                                    #
#                                                                             #
# brief:   Version1, use mean value of the intensity at specific areas to     #
#          see if there is a branch. It runs really slow because picamra      #
#          has to be turned on when each time it capture the picture.         #
#                                                                             #
###############################################################################


### Imports ###################################################################

import picamera
from PIL import Image
import numpy as np
from scipy.misc import imread, imsave
import matplotlib.image as mpimg
import RPi.GPIO as GPIO
import os 
from time import sleep


#### Detction of branch #######################################################
def detectBranch(site, value,thr):
    if site == 0 and (thr - value ) > 0.06:
        return True
    elif site == 1 and thr - value > 0.06:
        return True
    else:
        return False 
    
#### Function that receive a photo and make a decision ########################
def play(camera, thLU,thRU):
    # fileName: image name ; status: where to go ; 0 : go right, 1: go left
    status = 1 

    while True: 
        fileName = "image.jpg"
        camera_width = 320
        camera_height = 240
        camera.resolution = (camera_width, camera_height)
        camera.capture(fileName,format='jpeg')
        image_data = imread(fileName)
        #normalize image data to be between 0 -1 
        image_dataN=image_data/255.
        #Detect the branch by take average of intensity of the image at specific area
        leftUp   =np.mean(image_dataN[36:48, 25:80])
        rightUp  =np.mean(image_dataN[36:48, 150:200])
        if status == 0 :
            if detectBranch(0,leftUp,thLU): 
                status = 1
                doit(status)
            else : 
                status = 0
                doit(status)
        else:
            if detectBranch(1,rightUp,thRU): 
                status = 0
                pre = cur 
                doit(status)
            else : 
                status = 1
                doit(status)

#### GPIO output ##############################################################
def doit(status):
    if status == 1:
        print("go Left")
        GPIO.output(5,1)
        sleep(0.05)
        GPIO.output(5,0)
        
    else:
        print("go right")
        GPIO.output(6,1)
        sleep(0.05)
        GPIO.output(6,0)


#### Initialize PiCamera ######################################################
def main():
    #
    GPIO.setmode(GPIO.BCM)
    GPIO.setup(5,GPIO.OUT)
    GPIO.setup(6,GPIO.OUT)
    GPIO.output(5,0) 
    GPIO.output(6,0)
    
    print("Initializing the camera .....")
    #initialize picamera
    camera = picamera.PiCamera()
    # verizontal flip
    camera.vflip = True
    # Capture the first pic to determine the threshold
    camera_width = 320
    camera_height = 240
    camera.resolution = (camera_width, camera_height)
    camera.capture("threshold.jpg",format='jpeg')
    image_data = imread("threshold.jpg")
    camera.capture("threshold.jpg",format='jpeg')
    image_data = imread("threshold.jpg")
    image_dataN=image_data/255.

    thLU = np.mean(image_dataN[36:48, 25:80])
    thRU = np.mean(image_dataN[36:48, 150:200])
    print("Initialization done! And Start to Play!")
    play(camera, thLU,thRU)
    print("Game Over")
    

if __name__ == '__main__':
    main()
</code>
</pre>
</li>


<li><p><b><span>Second version - with highest scores as 2855 points </span></b></p>
            In this version we improve the perfomance of out Pi Game Player. In detail, the Pi Game Player is able to continuously tapping five times by the analyze of one photo thus the speed of playing this game is significantly increased. We also properly elongate the time for each tapping to guarantee the tapping accuracy.
<pre><code class="python">
###############################################################################
#                                                                             #
# file:    runOnPyFinal.py                                                    #
#                                                                             #
# authors: Wen-Yu Wang  - ww424                                               #
#          Hanchen Jin  - hj424                                               #
#                                                                             #
# date:    May 12 2016                                                        #
#                                                                             #
# brief:   This is our version7 code, using minimum intensity to detect if    #
#          there is a branch, and we also clean our code, making it clear.    #
#                                                                             #
###############################################################################


### Imports ###################################################################
import picamera
from PIL import Image
import numpy as np
from scipy.misc import imread, imsave
import matplotlib.image as mpimg
import RPi.GPIO as GPIO
import os 
from time import sleep
import io

### Main function including Setup #############################################

def main():

    #Setup for GPIO pins
    GPIO.setmode(GPIO.BCM)
    GPIO.setup(5,GPIO.OUT)
    GPIO.setup(6,GPIO.OUT)
    GPIO.output(5,0) 
    GPIO.output(6,0)

    # Capture the first pic to determine the threshold
    # Low resolution making it read file faster
    print("Initializing the camera .....")
    threshold =[[135] * 5 for i in range (2)]
    camera = picamera.PiCamera()
    camera.vflip = True
    camera_width = 160
    camera_height = 120
    camera.resolution = (camera_width, camera_height)
    for _ in range (5):
        camera.capture("threshold.jpg",format='jpeg',use_video_port=True)
    image_data = imread("threshold.jpg")
    
    threshold[0][0] = np.mean(image_data[90:98, 48:68,:])
    threshold[1][0] = np.mean(image_data[87:95, 89:107,:])



    print("Initialization done! And Start to Play!")
    play(camera,threshold)
    print("Game Over")


### Detect branch and move accordingly ########################################


def play(camera,threshold):
    # fileName: image name ; status: where to go ; 0 : go right, 1: go left
    status = True
    branch = [[0] * 5 for _ in range(2)]
    move = [False] * 5 


    while True: 
        fileName = "image.jpg"
        camera_width = 160
        camera_height = 120
        camera.resolution = (camera_width, camera_height)
        camera.capture(fileName,use_video_port=True)
        image_data = imread(fileName)
 
        #Detect the branch by take average of intensity of the image at specific area
        branch[0][0] = np.mean(image_data[90:98, 48:68,:])
        branch[1][0] = np.mean(image_data[87:95, 89:107,:])

        branch[0][1] = np.var(image_data[66:76, 45:60,:])
        branch[1][1] = np.var(image_data[65:73, 97:107,:])

        branch[0][2] = np.var(image_data[50:58, 45:60,:])
        branch[1][2] = np.var(image_data[48:57, 93:107,:])

        branch[0][3] = np.var(image_data[33:43, 40:52,:])
        branch[1][3] = np.var(image_data[32:46, 95:107,:])

        branch[0][4] = np.var(image_data[20:30, 45:57,:])
        branch[1][4] = np.var(image_data[20:32, 91:107,:])

        temp = status 
        for j in range (5):
            ##Detect branch and move
            if j == 0 :
                if (threshold[temp][0] - branch[temp][0]) > 55.:
                    temp = not temp 
            elif j in [1,2,3,4]:
                if branch[temp][j] >= threshold[temp][j] :
                    temp = not temp
            move[j]=temp

        status = temp
        for i in range (5):
            if move[i] is True : 
                GPIO.output(6,1)
                sleep(0.06)
                GPIO.output(6,0)
                sleep(0.09)
            else:
                GPIO.output(5,1)
                sleep(0.06)
                GPIO.output(5,0)
                sleep(0.09)

if __name__ == '__main__':
    main()
</code>
</pre>
</li>

<li><p><b><span>Tapping testing- controlled by GPIO output pulses </span></b></p>
            This is the script for testing the funtion of two tappings controlled by Raspberry Pi GPIO pins. Also it can perform the function of cleaning GPIO settings since the while loop in the main function can be only interrupted by "CTRL+C" which may lead to some problems on the next execution.  
<pre><code class="python">
###############################################################################
#                                                                             #
# file:    output_test_vfinal.py                                              #
#                                                                             #
# authors: Wen-Yu Wang  - ww424                                               #
#          Hanchen Jin  - hj424                                               #
#                                                                             #
# date:    May 1 2016                                                         #
#                                                                             #
# brief:   This is the final version of testing tapping function controlled   #
#          GPIO pulses                                                        #
#                                                                             #
###############################################################################
#import GPIO library, sleep function
import RPi.GPIO as GPIO
from time import sleep

#set GPIO mode  
GPIO.setmode(GPIO.BCM)
GPIO.setup(5,GPIO.OUT)
GPIO.setup(6,GPIO.OUT)
GPIO.output(5,0)
GPIO.output(6,0)

#set initial value
a=-1
while a &lt; 2:
#input 0,1 for testing tapping pads, other positive number for cleaning GPIO settings
	a= raw_input("Input a number:")
	a= int(a)
	if a==1:            #input 1 for testing one tapping pad	
		GPIO.output(6,1)
		sleep(0.05)     #tapping time required:0.05s
		GPIO.output(6,0)		
	elif a==0:          #input0 for testing another tapping pad	
		GPIO.output(5,1)
		sleep(0.05)     #tapping time required:0.05s
		GPIO.output(5,0)		
print("Control finishes!")
#clean up GPIO when normal exit  
GPIO.cleanup()
</code>
</pre>
</li>

</ul>
<br>
<p style="font-size:1.2em" ><b> References </b></p>
<a href="https://picamera.readthedocs.io/en/release-1.10/" target="_blank">PiCamera Document</a>
<br>
<a href="https://www.scipy.org/install.html" target="_blank">Scipy installation</a>
<br>
<a href="http://www.pythonware.com/products/pil/" target="_blank">Python Image Library Document</a>
<br>
<a href="http://effbot.org/imagingbook/" target="_blank">The Python Imaging Library Handbook</a>
<br>
<a href="https://people.ece.cornell.edu/land/courses/ece5760/FinalProjects/s2014/alt53_akt52_sm2354/alt53_akt52_sm2354/index.html" target="_blank">ECE 5760: Final Project: Flappy Bird Player</a>
<br>
<a href="http://pdf.datasheetcatalog.com/datasheet/fairchild/H11F1.pdf" target="_blank">H11F1 Datasheet</a>
<br>
<a href="http://startbootstrap.com/template-categories/one-page/" target="_blank">One-page bootstrap template</a>



           

        </div>

        <hr class="featurette-divider">

        <div class="featurette" id="Conclusion">
            <img class="featurette-image img-circle img-responsive pull-left" src="img/we.jpg" alt ="we">
            <h2 class="featurette-heading">At last, 
                <span class="text-muted">We want to say ...</span>
            </h2>
            <br>
            <br>
            <p class="lead">This project is lots of fun and challenging!! At first, the motivation for this project is that we want to build a system that can help us to play the game and get high scores. After developing this project, we found it challenging. The difficulties we faced includes environmental lighting condition, the stability of the tapping electrodes, the slow speed of pi-camera, the low-quality of images taken by pi-camera. We faced these difficulties and came up with the solutions accordingly, at last, we made it!! Combining the techniques of software and hardware, we help our raspberry create its eyes and hands. Finally, we made it can score almost 3000 points. </p>
            <br>
            <br>
            <br>
            <br>
            <p style="font-size:1.2em" ><b> Authors </b></p>
            <br>
            <img src = "img/wenyu.png" alt ="wenyu"> 
            <p style="font-size:1.2em" > Wen-Yu Wang (ww424) : <br>In charge of software part, including developing the algorithm for detection of branches (testing and debugging), cooperating with hardware part, and the template, formatting of the report. She loves this project and she thinks this project is the most interesting project she has worked during her student life. In this project, she thinks they have learned how to developing a "project" and how to solve problems systematically. )</p>
            <br>
            <img src = "img/hj424.png" alt="henchen"> 
            <p style="font-size:1.2em" > Hanchen Jin (hj424) : <br>In charge of hardware part, including the circuit design,testing and debugging. And he also provides some ideas and assistance for implementing the branches detecting algorithm. Actually at first, He thinks it is easy to build the circuit to tapping on the touch screen, however, selecting material to build the stable hardware which requires lots of experiments is really tough. Furthermore, he was impressed by the powerful Python libraries and he got some interests towards software development. </p>

        </div>
        <hr class="featurette-divider">

            <div class="featurette" id="acknowledgement">
            <img class="featurette-image img-circle img-responsive pull-right" src="img/CourseStaff.jpg" alt ="course staffs">
            <h2 class="featurette-heading">Acknowledgement 
            </h2>

            <p class="lead">Thanks professor and TAs for the helpful advices. In the process of developing this project, professor Joe gave us lots of useful information and cared about us very much. We really appreciated that. And the advices from TAs were helpful as well. Thanks for their efforts and made this project more perfect. Besides, Wen-Yu wanted to give a special thank to her boyfriend for the creative idea for this project :) </p>
            

        </div>
        <hr class="featurette-divider">

        <!-- Footer -->
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; Wen-Yu Wang (ww424) & Hanchen Jin (hj424) 2016</p>
                </div>
            </div>
        </footer>

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
