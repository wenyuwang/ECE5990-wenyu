<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="This is the report for ECE5990, final project">
    <meta name="author" content="Wen-Yu Wang">
    <link rel="icon" href="img/icon-2.png">
    <title>ECE5990Final-Project</title>
    <style type="text/css">
        ul {list-style-type: circle; font-size: 120%; line-height: 2}
        li span {font-size: 100%; vertical-align: middle;}
    </style>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/one-page-wonder.css" rel="stylesheet">
    <link rel="stylesheet" href="highlighJS/styles/default.css">
    <script src="highlighJS/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#">Pi Game Player </a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="#objectives">Objective</a>
                    </li>
                    <li>
                        <a href="#introduction">Introduction</a>
                    </li>
                    <li>
                        <a href="#setting_up">Set Up</a>
                    </li>
                    <li>
                        <a href="#System_Design">System Design</a>
                    </li>
                    <li>
                        <a href="#results">Results</a>
                    </li>
                    <li>
                        <a href="#Code_Appendix">Code Appendix</a>
                    </li>
                    <li>
                        <a href="#Conclusion">Conclusion</a>
                    </li>
                    <li>
                        <a href="#acknowledgement">Acknowledgement</a>
                    </li>

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Full Width Image Header -->
    <header class="header-image">
        <div class="headline">
            <div class="container">
                <h2>ECE5990 Final Project</h2>
                <h2>Pi Game Player for iPhone!</h2>
            </div>
        </div>
    </header>

    <!-- Page Content -->
    <div class="container">

        <hr class="featurette-divider">
        <!-- Zeroth Featurette -->
        <div class="featurette" id="objectives">
            <img class="featurette-image img-circle img-responsive pull-left" src="img/ourpi.jpg">
            <h2 class="featurette-heading">What Goals
                <span class="text-muted">We want</span>
            </h2>
            <br>
            <br>
            <br>
            <p class="lead"> - Build a robotic system to play mobile game app </p>
            <p class="lead"> - High accuracy (high scores) is guaranteed</p>
            <p class="lead"> - High speed to play the game apps is also guaranteed </p>
            <br>
            <br>


        </div>

        <hr class="featurette-divider">

        <!-- First Featurette -->
        <div class="featurette" id="introduction">
            <img class="featurette-image img-circle img-responsive pull-right" src="img/game.jpg">
            <h2 class="featurette-heading">Get to know
                <span class="text-muted">Our Idea</span>
            </h2>
            <p class="lead">Raspberry Pi is an ideal platform to build a small robot. We get the idea from Lab3 which we can build a robotic car on a raspberry pi system by writing some controlling programming. But, in Lab3, our robot does not have “eyes”. So, here is our idea – we want to build up a robot with eyes and make it play some simple game apps, such as Lumberman. Lumberman is a mobile game app, where you need to avoid the branched when you chop the wood. We implemented the algorithm to detect where are thr branches and make our raspberry pi to chop the wood! </p>
            <br>
            
            <font size="4">Keyword: PiCamera, Image Processing, Raspberry Pi </font>

        </div>

        <hr class="featurette-divider">

        <!-- Second Featurette -->
        <div class="featurette" id="setting_up">
            <h2 class="featurette-heading">Setup environment
                <span class="text-muted">For PiCamera and Libraries</span>
            </h2>
            <p class="lead">Before we start our project, we need to set up the environment for raspberry pi.</p>
            <ul>

            <p><b><li><span>PiCamera</span></li></b></p>
            <font style="background-color:#E0E0E0">python-picamera</font> is a pure Python interface to the Raspberry Pi camera module for Python 2.7 (or above) or Python 3.2 (or above). 

            <p><b>First: Enable PiCamera</b>
            <br>
            Run <font style="background-color:#E0E0E0">sudo raspi-config</font> and choose in the menu to enable the pi camera. A reboot is needed after this.</p>

            <p><b>Second: Install Library for PiCamera</b>
            <br>
            Run the following commands to get the library.</p>
            <blockquote>
                $ sudo apt-get update <br>
                $ sudo apt-get install python-picamera
            </blockquote>

            Now you can import piCamera and capture a picture!
            <blockquote>

                <font color = "#0076ac">import</font> picamera <br>
                camera = picamera.PiCamera() <br>
                camera.capture(<font color = "#01B468">'image.jpg'</font>)

            </blockquote>

            <p><b><li><span>Image Processing Related Libraries</span></li></b></p>
            Since we need our raspberry pi to "see" a scene, and we need the image-processing algorithm to help us to detect if there is a branch at specific site or not. And we need to install the libraries including <font style="background-color:#E0E0E0">SciPy</font>, <font style="background-color:#E0E0E0">Python Image Library (PIL)</font>,  <font style="background-color:#E0E0E0">matlibplot</font> , and <font style="background-color:#E0E0E0">skimage</font>.

            <p><b>Scipy/NumPy/matplotlib</b>
            <br>
            The SciPy library is one of the core packages that make up the SciPy stack. It provides many user-friendly and efficient numerical routines such as routines for numerical integration and optimization.
            <br>
            <br>
            NumPy is the fundamental package for scientific computing with Python. It contains among other things:
                <br>- a powerful N-dimensional array object
                <br>- sophisticated (broadcasting) functions
                <br>- tools for integrating C/C++ and Fortran code
                <br>- useful linear algebra, Fourier transform, and random number capabilities
                <br>Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.
            <br>
            <br>
            matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. </p>
            <blockquote>
                $ sudo apt-get install python-numpy python-scipy python-matplotlib python-pandas python-sympy python-nose 
            </blockquote>

            <p><b>Python Imaging Library (PIL)</b>
            <br>
            The Python Imaging Library (PIL) adds image processing capabilities to your Python interpreter. This library supports many file formats, and provides powerful image processing and graphics capabilities.
            <br>
            Here we are going to install PIL using pip, so we start with installing python-pip.

            <blockquote>
                $ sudo apt-get install python-pip
            </blockquote>
            With pip installed, install the required development packages:
            <blockquote>
                $ sudo apt-get install python-dev libjpeg-dev libfreetype6-dev zlib1g-dev
            </blockquote>
            After installing these packages, we have to symlink the three image libraries into /usr/lib. A symlink, which is short for symbolic link, is a special type of file that contains a reference to another file or directory. The reference is in the form of an absolute or relative path and it affects path-name resolution. To do that, type in the following commands on the terminal:
            <blockquote>
                $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libfreetype.so /usr/lib/
                <br>
                $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libjpeg.so /usr/lib/
                <br>
                $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libz.so /usr/lib/
            </blockquote>
            Now we are ready to install PIL. Type the following:
             <blockquote>
                $ sudo pip install pil
            </blockquote>
            To install Pillow (recommended), type the following:
            <blockquote>
                $ sudo pip install Pillow
            </blockquote>
            </p>
            </ul>

        </div>

        <hr class="featurette-divider">

        <!-- Third Featurette -->
        <div class="featurette" id="System_Design">
            <img class="featurette-image img-circle img-responsive pull-left" src="img/platform0.jpg">
            <h2 class="featurette-heading">Sofeware and Hardware
                <span class="text-muted">work together :)</span>
            </h2>
            <br>
            <p class="lead">Our project can be splitted to three categories: platform design, hardware design and software design. Platform design is to provide the spaces for our smartphone and the raspberry pi, also make sure the relative postion of the camera and our smartphone can be fixed. As for hardware design, we are building the "hands" for the raspberry pi to invoke the touch events on the capacity screen of the smartphone when it needs to move to the other side. Last but not the least, the software part- this part can be analogous as the mind of our raspberry pi. We used the image-processing algorithm to detect the branchs and decide where to chop the woods on the screen. This project is lots of fun! Join us and see what each part works by scrolling down!  </p>
            <br>
            <br>
            <br>

            <ul>

            <p><b><li><span>Platform Design</span></li></b></p>
            For platform design, we used LEGO to build our architecture. With the flexibility of LEGO, we can rebuild the platform anytime we want when it's not fit for our project. On this platform, we built several spaces for different parts. Mobile phone, raspberry pi and the breadboard all have their specific areas to be at, and it can help us to fix the relative distance from the picamera and mobile phone. Therefore, we can eliminate some uncertainties and make it more easier to implement the image processing algorithm.  
            <br>
            <br>
            <div style="text-align:center">
            <img src="img/platform1.jpg">
            <br>
            <br>
            <img src="img/platform2.jpg">
            <img src="img/platform3.jpg">
            <img src="img/platform4.jpg">
            </div>
            <p><b><li><span>Software Design</span></li></b></p>
            <div style="text-align:center">
            <img src="img/schematicForsoft.jpg">
            </div>
            The flow-chart shown above is the basic idea for software part to detect the branch and tap the screen of our mobile phone. At the initialization stage, we take 5 pictures in a row to make sure the lighting condition and white balance setting are stable for piCamera. We use the fifth photo we take to determine the threshold for our detection part. Actually, we have improved the performance of our algorithms by implementing different versions of our codes, so I will go through each version and highlight the significant improvement in each step.
            <br> 
            <br>
            <p style="margin-left:1em;">
            <b>- First version with highest scores 40  </b>
            <br>
            The first version of our design is the slowest algorithm but with high accuracy. The idea is quite simple. Among all the steps in our implementation, deciding the values of a threshold is the trickiest part for our algorithm. In our first version of our algorithm, we stored the <font style="background-color:#E0E0E0">mean values</font> of intensities over the region of the first branches right above the head of the lumberman. The reason we use the mean values is because that at the beginning of the game, there is definitely no branch right above the head of the lumberman. Besides, the region right above the head of the lumberman contains lots of high-intensity values (white color with intensity 255), then the mean value at the site is high enough to differentiate the mean values for the intensities when there appears a branch (with green, black color which lower the intensity of the area). Therefore, we just need to calculate the differences between the mean values of the intensities of the specific region with the stored values at the beginning, and if the difference is larger than some value (we set 60 here) then we can know there is a branch. 
            <br>
            We used the <font style="background-color:#E0E0E0">time</font> module in Python as the timer to calculate how long does each function take when it executes in our algorithm to find out the bottleneck step which takes the most time. After such operation, we found that the time for PiCamera to take a picture requires 0.2-0.4 secs while the calculation of the mean values and differences only takes a few milliseconds. In order to speed up the speeds of the capturing photos, we set the parameter <font style="background-color:#E0E0E0">use_video_port</font> <font color = "#7700BB">True</font> in the capturing method. However, unfortunately after this small improvement, it helped quite well - it can achieve 40 points (previous 14 points), but the time is still too long.
            <blockquote>
                camera.capture(fileName, <i><font color = "#ff8800">use_video_port</font></i>=<font color = "#7700BB">True</font>)
            </blockquote>
            <br>
            </p>
            <p style="margin-left:1em;">
            <b>- Second version</b>
            <br>
            In the Lumberman game app, time is a key factor to get high scores. When you response too late, the remaining time decays much quicker while if you tap quickly and correctly the remaining time will increase. So we did lots of improvements to speed up. Here are some efforts we've tried.
            <br>
            <p style="margin-left:2em;">
            <b>Lower the resolution of the picture took</b>
            <br>
            We lowered the resolution then it can help to eliminate the number of pixels we need to consider about and make it faster to write in a file and read the image file. The resolution of the pictures for our latest version is 160 * 120.
            </p>
            <p style="margin-left:2em;">
            <b>Isolate a CPU core and try to eliminate latency from CPU</b>
            <br>
            We referred to LectureNote 35 and tried to reduce the latency from CPU by isolating a CPU core. We set the configuration like the instruction following:
            <blockquote>
                Add 'isolcpus=3' to /boot/cmdline.txt
            </blockquote>
            </p>
            <p style="margin-left:2em;">
            After that, everytime we run our python script, we type the following to run our code.
            <blockquote>
                Taskset –c 3 python fileName.py
            </blockquote></p>
            <p style="margin-left:2em;">
            This is a good way to reduce the latency from the system and it can help to speed up for 0.03 - 0.04 seconds.
            </p>
            <p style="margin-left:2em;">
            <b>Using in-memory streams to store the picture instead of storing in SD card.</b>
            <br>
            The other problem we think we encountered is SD card speed limitations. In other words, we  need to capture to something faster like a network port or in-memory streams. So we utilized the <font style="background-color:#E0E0E0">stream = io.BytesIO()</font> and turned the stream to PIL (Python Image Library) object for further image-processing. Although it can help speed up for a little (0.03 faster), the time is still too long.
            <blockquote>
                stream = io.BytesIO()
                <br>
                camera.capture(stream, <i><font color = "#ff8800">format</font></i>=<font color = "#01B468">'jpeg'</font>,<i><font color = "#ff8800">use_video_port</font></i>=<font color = "#7700BB">True</font>)
                <br>
                stream.seek(<font color = "#ff2610">0</font>)
                <br>
                image = Image.open(stream).convert(<i><font color = "#01B468">'L'</font></i>)
                <br>
                arr = np.fromiter(<font color = "#1c96f7">iter</font>(image.getdata()), np.uint8)
                <br>
                arr.resize(<font color = "#ff2610">120</font>, <font color = "#ff2610">160</font>)
            </blockquote> 
            </p>
            <p style="margin-left:2em;">
            <b>Tap several times after taking a picture (most significant)</b> <img src="img/star.png"><img src="img/star.png">
            <br>
            This idea is inspired by the generous TA - Jinyao Ren. He suggested that we can try to tap three times when we take a picture and it can help to speed up three times. We took his good advice and implement the algorithm to detect five branches at a time and tap five times accordingly. In this implementation, we encountered several challenges. 
            <div style="text-align:center">
            <img src="img/gameLayout.jpg">
            </div>
            The image shown above is the screenshot of Lumberman app. As you can see here, when we take the first image to determine the threshold values, we have to consider 10 sites and set thresholds for each site. But, how? First, we need to know if there is a branch at each area or not. Second, based on our analysis of the intensities of the images taken by PiCamera, the intensity values can vary a lot owing to the environmental lighting condition. Which results in different intensity values between not only different layers but also left-hand side and right-hand side. Therefore, as a small summary here, we need to assign different thresholds for each site. We tried different methods to determine the thresholds and improve accuracy. Like using minimum values - because if there is a branch, it would contain the black color in the area and the intensity is 0. However as we said earlier in this session, due to various environment lighting condition, the intensities at different regions are not consistent. Using minimum or maximum can not reach high accuracy for detection a branch. At last, we came up with a great idea about this, that is using <font style="background-color:#E0E0E0">"variance"</font> values over the regions for branches. Since in the case that if there is no branch at site 1,2,3,4, the background is uniform, and the variance of the intensity values at these areas is small. When there exists a branch, the variance of the intensity values can by a large value! So in lastet version of our design, the detection method for site 0 is to find out the <font style="background-color:#E0E0E0">differences</font> between the original image and each image we take for tapping event; the detection method for site 1,2,3,4 is to use <font style="background-color:#E0E0E0">variances</font> as our threshold - if the variance at the specific area is larger than some value (we set 300 here) and it means there exists a branch. Furthermore, to fit this new detection method, we need to carefully decide the region we want to detect due to some unexpected situations - such as the screen pops up "Level x" when you reach Level x and the time-remaining strip affects the detection as well. The image shown below is the regions we decide to detect the branches.
            <div style="text-align:center">
            <img src="img/gamelayout2.jpg">
            </div>
            With this modified detection method and good hardware part for tapping the capacitive screen of our mobile phone, our raspberry pi can score with 2855 - a really high new record! But there is still another issue here, that is the threshold setting is optimally designed for the red background of the game. This game contains three kinds of background, it is required to have different detection method for our algorithm. 
            </p>
            <br>
            <div style="text-align:center">
            <font size = "5"> Come join us and see if you can beat our raspberry pi!! </font>
            </div>
            </p>
			<p><b><li><span>Hardware Design</span></li></b></p>
			So after the Pi Game player finish the process of taking photos and detecting branches, it is time to build "hands" for it to tap on the screen. Instead of using expensive robetic arms, we build circuits using eletrical switches to trigger the tapping on the touch screen on our cellphone. Key points are building pads for contacting with the touch screen, circuits for controlling the tapping by nomrmal switches, circuits for controlling the tapping by GPIO pulses and the test of the minimum tapping time.
			<br>
			(1) Tapping pads
			<br>
			The basic principle for tapping on the capacitive touch screen is to draw small amount of eletrons from the screen. For example, when people touch the screen with fingers, they are able to draw some electrons by the circuit loop from the cell phone to the ground since people has a capacitance about 30pF~50pF. Thus for triggering a tap on the capacitive touch screen, conductor with a certain magnitude of capacitance is required.
			<br>
			Firstly we use several layers of copper tap to build the tapping pads. After many experienments, we found that the capcitance of ten layers copper tap is good to trigger the tapping accurately. However, a deadly defect of the copper tap pad is that it is unstable and hardly be reuesd. It performs really good at the first time, but next time we re-stick the pad on the screen, it is not as sensitive as before. The reason is obvious. The process of tearing off from the screen slightly changes the space among each layers thus intensely alters its capacitance. It is quite annoying that we have to re-build two pads for each labs. So we decide to find another stable methods to build these two pads.
			<br>
			Then we found the one cent coin (copper foil) can also serve as a good pad for triggering the touch. Furthermore, within the wire soldered on it, it contacts really well and very stable. Here is picture which shows these two kinds of pads, the left one is built by the copper tap and the right one is built by the one cent coin. One tricky thing here is that the distance between the pad and the screen should be very small. So everytime when we restart our work, we have to bind the pads tightly on the screen by powerful tapes or else the sensitivity will be severely influenced which leads to higher probability of miss touch.
			<br>
				<div style="text-align:center">
                <img src = "img/electrodes.jpg">
                </div>
			<br>
			(2) Circuits using normal switches
			<br>
			After resolving the problem of stable pads for touching the screen, a simple current loop for testing its basic function is essential. Since the other side of the pad has to connecte to the ground (the back of the cell phone), a ground platform made by tinfoil with the same size of our cell phone helps the connection bwtween the other side of the wire and the back of our cell phone. As the following picture shows, with the proper phone case, it contacts pretty good.Then the pad and the ground is connected separately on the two side of a switch. By successfully trigger the tapping by normal switches, it is time to move on eletric switches.
			<br>
				<div style="text-align:center">
                <img src = "img/ground.jpg">
                </div>
			<br>
			(3) Eletric switches
			<br>
			
			<br>
			(4) Tapping time requirement
			

            </ul>
			

        </div>

        <hr class="featurette-divider">
        <!-- Fourth Featurette -->
        <div class="featurette" id="results">
            <img class="featurette-image img-circle img-responsive pull-right" src="img/result1.jpg">
            <h2 class="featurette-heading">How Good
                <span class="text-muted">We Achieved</span>
            </h2>
            <p class="lead">Raspberry Pi is an ideal platform to build a small robot. We get the idea from Lab3 which we can build a robotic car on a raspberry pi system by writing some controlling programming. But, in Lab3, our robot does not have “eyes”. So, here is our idea – we want to build up a robot with eyes and make it play some simple game apps, such as Lumberman. Lumberman is a mobile game app, where you need to avoid the branched when you chop the wood. We implemented the algorithm to detect where are thr branches and make our raspberry pi to chop the wood! </p>
            <br>
            
            <p> Keyword: PiCamera, Image Processing, Raspberry Pi </p>

        </div>

        <hr class="featurette-divider">

                <!-- Fourth Featurette -->
        <div class="featurette" id="Code_Appendix">
            <h2 class="featurette-heading">What we have created
                <span class="text-muted">For this project</span>
            </h2>
            <p class="lead">Here is the code appendix for our project. We've created several versions of codes here.</p>
            <br>

            <font size = "4"><b> Parts List </b></font>

             <table style="border: 2px solid rgb(148, 201, 255); height: 120px; background-color: rgb(242, 247, 247); width: 350px;" align="center" cellpadding="2" cellspacing="5" frame="border" rules="row">
                <tbody>
                    <tr>
                        <td><b> Name</b></td>
                        <td><b>Price</b></td>
                        <td><b>Quantity</b></td>
                    </tr>
                    <tr>
                        <td> Raspberry Pi2 B+ </td>
                        <td>35$</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td> PiCamera Board Module</td>
                        <td>19$</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td> Wires</td>
                        <td>Stock in Lab</td>
                        <td>some</td>
                    </tr>
                    </tbody>
            </table>

            <br>
            <br>
            <font size = "4"><b> Codes </b></font>
            <ul>

            <p><b><li><span>First version - with highest scores as 14 points </span></li></b></p>
            This version is our first trial, which runs slowly. The main bottle neck is the time for taking a picture. The progress of this algortithm is - take a picture, image-processing and then trigger the touch event. However, the picamera is initialized and turned on its configuration each time when it needs to capture a picture. Therefore it went slow.

<pre><code class="python">
###############################################################################
#                                                                             #
# file:    runOnPy.py                                                         #
#                                                                             #
# authors: Wen-Yu Wang  - ww424                                               #
#          Hanchen Jin  - hj424                                               #
#                                                                             #
# date:    April 30th 2016                                                    #
#                                                                             #
# brief:   Version1, use mean value of the intensity at specific areas to     #
#          see if there is a branch. It runs really slow because picamra      #
#          has to be turned on when each time it capture the picture.         #
#                                                                             #
###############################################################################


### Imports ###################################################################

import picamera
from PIL import Image
import numpy as np
from scipy.misc import imread, imsave
import matplotlib.image as mpimg
import RPi.GPIO as GPIO
import os 
from time import sleep


#### Detction of branch #######################################################
def detectBranch(site, value,thr):
    if site == 0 and (thr - value ) > 0.06:
        return True
    elif site == 1 and thr - value > 0.06:
        return True
    else:
        return False 
    
#### Function that receive a photo and make a decision ########################
def play(camera, thLU,thRU):
    # fileName: image name ; status: where to go ; 0 : go right, 1: go left
    status = 1 

    while True: 
        fileName = "image.jpg"
        camera_width = 320
        camera_height = 240
        camera.resolution = (camera_width, camera_height)
        camera.capture(fileName,format='jpeg')
        image_data = imread(fileName)
        #normalize image data to be between 0 -1 
        image_dataN=image_data/255.
        #Detect the branch by take average of intensity of the image at specific area
        leftUp   =np.mean(image_dataN[36:48, 25:80])
        rightUp  =np.mean(image_dataN[36:48, 150:200])
        if status == 0 :
            if detectBranch(0,leftUp,thLU): 
                status = 1
                doit(status)
            else : 
                status = 0
                doit(status)
        else:
            if detectBranch(1,rightUp,thRU): 
                status = 0
                pre = cur 
                doit(status)
            else : 
                status = 1
                doit(status)

#### GPIO output ##############################################################
def doit(status):
    if status == 1:
        print("go Left")
        GPIO.output(5,1)
        sleep(0.05)
        GPIO.output(5,0)
        
    else:
        print("go right")
        GPIO.output(6,1)
        sleep(0.05)
        GPIO.output(6,0)


#### Initialize PiCamera ######################################################
def main():
    #
    GPIO.setmode(GPIO.BCM)
    GPIO.setup(5,GPIO.OUT)
    GPIO.setup(6,GPIO.OUT)
    GPIO.output(5,0) 
    GPIO.output(6,0)
    
    print("Initializing the camera .....")
    #initialize picamera
    camera = picamera.PiCamera()
    # verizontal flip
    camera.vflip = True
    # Capture the first pic to determine the threshold
    camera_width = 320
    camera_height = 240
    camera.resolution = (camera_width, camera_height)
    camera.capture("threshold.jpg",format='jpeg')
    image_data = imread("threshold.jpg")
    camera.capture("threshold.jpg",format='jpeg')
    image_data = imread("threshold.jpg")
    image_dataN=image_data/255.

    thLU = np.mean(image_dataN[36:48, 25:80])
    thRU = np.mean(image_dataN[36:48, 150:200])
    print("Initialization done! And Start to Play!")
    play(camera, thLU,thRU)
    print("Game Over")
    

if __name__ == '__main__':
    main()
</code>
</pre>


             <p><b><li><span>Second version - with highest scores as 2855 points </span></li></b></p>
            In this version we improve our speed for pi to play this game app, also high accuracy is guaranteed. 
<pre><code class="python">
###############################################################################
#                                                                             #
# file:    runOnPyFinal.py                                                    #
#                                                                             #
# authors: Wen-Yu Wang  - ww424                                               #
#          Hanchen Jin  - hj424                                               #
#                                                                             #
# date:    May 12 2016                                                        #
#                                                                             #
# brief:   This is our version7 code, using minimum intensity to detect if    #
#          there is a branch, and we also clean our code, making it clear.    #
#                                                                             #
###############################################################################


### Imports ###################################################################
import picamera
from PIL import Image
import numpy as np
from scipy.misc import imread, imsave
import matplotlib.image as mpimg
import RPi.GPIO as GPIO
import os 
from time import sleep
import io

### Main function including Setup #############################################

def main():

    #Setup for GPIO pins
    GPIO.setmode(GPIO.BCM)
    GPIO.setup(5,GPIO.OUT)
    GPIO.setup(6,GPIO.OUT)
    GPIO.output(5,0) 
    GPIO.output(6,0)

    # Capture the first pic to determine the threshold
    # Low resolution making it read file faster
    print("Initializing the camera .....")
    threshold =[[135] * 5 for i in range (2)]
    camera = picamera.PiCamera()
    camera.vflip = True
    camera_width = 160
    camera_height = 120
    camera.resolution = (camera_width, camera_height)
    for _ in range (5):
        camera.capture("threshold.jpg",format='jpeg',use_video_port=True)
    image_data = imread("threshold.jpg")
    
    threshold[0][0] = np.mean(image_data[90:98, 48:68,:])
    threshold[1][0] = np.mean(image_data[87:95, 89:107,:])



    print("Initialization done! And Start to Play!")
    play(camera,threshold)
    print("Game Over")


### Detect branch and move accordingly ########################################


def play(camera,threshold):
    # fileName: image name ; status: where to go ; 0 : go right, 1: go left
    status = True
    branch = [[0] * 5 for _ in range(2)]
    move = [False] * 5 


    while True: 
        fileName = "image.jpg"
        camera_width = 160
        camera_height = 120
        camera.resolution = (camera_width, camera_height)
        camera.capture(fileName,use_video_port=True)
        image_data = imread(fileName)
 
        #Detect the branch by take average of intensity of the image at specific area
        branch[0][0] = np.mean(image_data[90:98, 48:68,:])
        branch[1][0] = np.mean(image_data[87:95, 89:107,:])

        branch[0][1] = np.var(image_data[66:76, 45:60,:])
        branch[1][1] = np.var(image_data[65:73, 97:107,:])

        branch[0][2] = np.var(image_data[50:58, 45:60,:])
        branch[1][2] = np.var(image_data[48:57, 93:107,:])

        branch[0][3] = np.var(image_data[33:43, 40:52,:])
        branch[1][3] = np.var(image_data[32:46, 95:107,:])

        branch[0][4] = np.var(image_data[20:30, 45:57,:])
        branch[1][4] = np.var(image_data[20:32, 91:107,:])

        temp = status 
        for j in range (5):
            ##Detect branch and move
            if j == 0 :
                if (threshold[temp][0] - branch[temp][0]) > 55.:
                    temp = not temp 
            elif j in [1,2,3,4]:
                if branch[temp][j] >= threshold[temp][j] :
                    temp = not temp
            move[j]=temp

        status = temp
        for i in range (5):
            if move[i] is True : 
                GPIO.output(6,1)
                sleep(0.06)
                GPIO.output(6,0)
                sleep(0.09)
            else:
                GPIO.output(5,1)
                sleep(0.06)
                GPIO.output(5,0)
                sleep(0.09)

if __name__ == '__main__':
    main()
</code>
            </pre>
            </ul>
           

        </div>

        <hr class="featurette-divider">

        <div class="featurette" id="Conclusion">
            <img class="featurette-image img-circle img-responsive pull-left" src="img/we.jpg">
            <h2 class="featurette-heading">At last, 
                <span class="text-muted">We want to say ...</span>
            </h2>
            <br>
            <br>
            <br>
            <br>
            <br>
            <br>
            <br>
            <br>
            <p class="lead">This project is lots of fun and challenging!! At first, the motivation for this project is that we want to build a system that can help us to play with the game and get high scores. </p>
            <br>
            <br>
            <br>
            <br>
            <br>
            <br>
            <br>
            <br>
            <font size = "4"><b> Authors </b></font>
            <br>
            <br>
            <img src = "img/wenyu.png"> 
            <font size = "4"> Wen-Yu Wang (ww424) : <br>In charge of software part, including the detection of branches and the template of report. She loves this project and she thinks this project is the most interesting project she has worked during her student life. In this project, she thinks they have learned how to developing a "project" and how to solve problems systematically. )</font>
            <br>
            <img src = "img/hj424.png"> 
            <font size = "4"> Hanchen Jin (hj424) : <br>In charge of hardware part, including the circuit design and the connection of the GPIO. </font>

        </div>
        <hr class="featurette-divider">

            <div class="featurette" id="acknowledgement">
            <img class="featurette-image img-circle img-responsive pull-right" src="img/CourseStaff.jpg">
            <h2 class="featurette-heading">Acknowledgement 
            </h2>

            <p class="lead">Thanks professor and course staffs for the helpful advices. In the process of developing this project, professor Joe gave us lots of useful information and cared about us very much. We really appreciated that. And the advices from TAs were helped as well. Thanks for your efforts and made this project more perfect. Besides, Wen-Yu wanted to give a special thank to her boyfriend for the creative idea for this project :) </p>
            

        </div>
        <hr class="featurette-divider">

        <!-- Footer -->
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; Wen-Yu Wang (ww424) & Hanchen Jin (hj424) 2016</p>
                </div>
            </div>
        </footer>

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
